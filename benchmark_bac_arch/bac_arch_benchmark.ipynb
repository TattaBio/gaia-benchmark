{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import torch\n",
    "import faiss\n",
    "from transformers import AutoModel, AutoTokenizer, EsmForProteinFolding\n",
    "datasets.disable_caching()\n",
    "faiss.omp_set_num_threads(4)\n",
    "\n",
    "\n",
    "MMSEQS_PATH = \"/path/to/mmseqs2\"  # Replace with the actual path to your mmseqs2 installation\n",
    "FOLDSEEK_PATH = \"/path/to/foldseek\"  # Replace with the actual path to your Foldseek installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "dataset = datasets.load_dataset(\"tattabio/bac_arch_bigene\", keep_in_memory=True)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bac_test.fasta\", \"w\") as fasta_file:\n",
    "    for example in dataset:\n",
    "        seq = example[\"Seq1\"]\n",
    "        accession = example[\"Seq1_accession\"]\n",
    "        fasta_file.write(f\">{accession}\\n{seq}\\n\")\n",
    "\n",
    "with open(\"arc_test.fasta\", \"w\") as fasta_file:\n",
    "    for example in dataset:\n",
    "        seq = example[\"Seq2\"]\n",
    "        accession = example[\"Seq2_accession\"]\n",
    "        fasta_file.write(f\">{accession}\\n{seq}\\n\")\n",
    "\n",
    "correct_matches = {}\n",
    "for example in tqdm(dataset):\n",
    "    seq = example[\"Seq1\"]\n",
    "    accession = example[\"Seq1_accession\"]\n",
    "    correct_match = example[\"Seq2_accession\"]\n",
    "    correct_matches[accession] = correct_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BlastP benchmark\n",
    "blast_command = f\"blastp -query bac_test.fasta -subject arc_test.fasta -outfmt 6 -out blast_results.txt\"\n",
    "start = time.time()\n",
    "os.system(blast_command)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print(f\"BlastP Time taken: {time_taken} seconds\")\n",
    "\n",
    "correct = 0\n",
    "seen = set()  # to keep track of already seen queries   \n",
    "with open(f\"blast_results.txt\", \"r\") as blast_file:\n",
    "    # for k=1 to 10 check if the top hit is included\n",
    "\n",
    "    lines = blast_file.readlines()\n",
    "   \n",
    "    for line in lines: \n",
    "        query = line.split()[0]\n",
    "        if query not in seen: \n",
    "            hit = line.split()[1]\n",
    "            correct_match = correct_matches.get(query, None)\n",
    "            if correct_match == hit:\n",
    "                correct += 1\n",
    "        seen.add(query)\n",
    "# calculate recall \n",
    "recall = correct / len(correct_matches)\n",
    "print(f\"BlastP recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mmseqs benchmark\n",
    "correct = 0\n",
    "start = time.time()\n",
    "os.system(f\"{MMSEQS_PATH} createdb arc_test.fasta arc_test_db\")\n",
    "mmseqs_command = f\"{MMSEQS_PATH} easy-search bac_test.fasta arc_test_db mmseqs_results.m8 tmp --threads 4 -v 0\"\n",
    "os.system(mmseqs_command)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print(f\"Mmseqs Time taken: {time_taken} seconds\")\n",
    "\n",
    "correct = 0 \n",
    "seen = set()\n",
    "\n",
    "with open(f\"mmseqs_results.m8\", \"r\") as mmseqs_file:\n",
    "    # if the file is empty, skip to the next iteration\n",
    "    lines = mmseqs_file.readlines()\n",
    "    for line in lines:\n",
    "        query = line.split()[0]\n",
    "        if query not in seen:\n",
    "            hit = line.split()[1]\n",
    "            correct_match = correct_matches.get(query, None)\n",
    "            if correct_match == hit:\n",
    "                correct += 1\n",
    "        seen.add(query)\n",
    "        \n",
    "# calculate recall\n",
    "recall = correct / len(correct_matches)\n",
    "print(f\"Mmseqs recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protein structure prediction for Foldseek using ESMFold\n",
    "def standardize_amino_acids(sequence: str) -> str:\n",
    "    modified_amino_acid_map = {\n",
    "        \"O\": \"K\",  # Pyrrolysine -> Lysine\n",
    "        \"U\": \"C\",  # Selenocysteine -> Cysteine\n",
    "        \"J\": \"L\",  # Isoleucine/Leucine -> Leucine\n",
    "    }\n",
    "    for modified_amino_acid, standard_amino_acid in modified_amino_acid_map.items():\n",
    "        sequence = sequence.replace(modified_amino_acid, standard_amino_acid)\n",
    "    return sequence\n",
    "\n",
    "class ESMFoldModel:\n",
    "    MODEL_NAME = \"facebook/esmfold_v1\"\n",
    "    NUM_RECYCLES = 4\n",
    "\n",
    "    def __init__(self, device = 'cuda'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL_NAME)\n",
    "        self.model = EsmForProteinFolding.from_pretrained(self.MODEL_NAME).eval().to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def inference(self, protein_str):\n",
    "        protein_str = standardize_amino_acids(protein_str)\n",
    "        input_ids = self.tokenizer([protein_str], return_tensors=\"pt\", add_special_tokens=False)['input_ids'].to(self.device)\n",
    "        with torch.inference_mode(), torch.autocast(\n",
    "            device_type=\"cuda\", dtype=torch.float16, \n",
    "        ):\n",
    "            output = self.model(input_ids, num_recycles=self.NUM_RECYCLES)\n",
    "        pdb = self.model.output_to_pdb(output)[0]\n",
    "        return pdb\n",
    "\n",
    "def write_pdb_str_to_file(pdb_str: str, file_path: str):\n",
    "    with open(file_path, 'w') as f:\n",
    "        # Replace the ESMFold header with the expected header.\n",
    "        pdb_str = pdb_str.replace('PARENT N/A', '')\n",
    "        f.write('CRYST1    1.000    1.000    1.000  90.00  90.00  90.00 P 1           1          ')\n",
    "        f.write(pdb_str)\n",
    "    \n",
    "\n",
    "esmfold_model = ESMFoldModel(device='cuda')\n",
    "\n",
    "os.makedirs('structures_seq1', exist_ok=True)\n",
    "os.makedirs('structures_seq2', exist_ok=True)\n",
    "\n",
    "def process_row(row):\n",
    "    # Skip if PDB files already exist for this pair\n",
    "    seq1_id, seq2_id = row['Seq1_accession'], row['Seq2_accession']\n",
    "    if os.path.exists(f'structures_seq1/{seq1_id}.pdb') and os.path.exists(f'structures_seq2/{seq2_id}.pdb'):\n",
    "        return\n",
    "    seq1_pdb = esmfold_model.inference(row['Seq1'])\n",
    "    seq2_pdb = esmfold_model.inference(row['Seq2'])\n",
    "    write_pdb_str_to_file(seq1_pdb, f'structures_seq1/{seq1_id}.pdb')\n",
    "    write_pdb_str_to_file(seq2_pdb, f'structures_seq2/{seq2_id}.pdb')\n",
    "\n",
    "start_time = time.time()\n",
    "_ = dataset.map(process_row)\n",
    "end_time = time.time()\n",
    "print(f\"Structure prediction (ESMFold) for Foldseek Time taken: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foldseek benchmark\n",
    "total_time = 0\n",
    "correct = 0\n",
    "start = time.time()\n",
    "createdb_command1 = f\"/mnt/raid0/apps/foldseek/bin/foldseek createdb structures2 targetDB\"\n",
    "createdb_command2 = f\"/mnt/raid0/apps/foldseek/bin/foldseek createdb structures1 queryDB\"\n",
    "os.system(createdb_command1)\n",
    "os.system(createdb_command2)\n",
    "foldseek_command = f\"/mnt/raid0/apps/foldseek/bin/foldseek easy-search queryDB targetDB foldseek_results.m8 /tmp --threads 4 -v 0\"\n",
    "os.system(foldseek_command)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "seen = set()\n",
    "\n",
    "with open(f\"foldseek_results.m8\", \"r\") as foldseek_file:\n",
    "\n",
    "    lines = foldseek_file.readlines()\n",
    "    for line in lines:\n",
    "        # for each line, check if the query has already been seen\n",
    "        # if it has, skip to the next line\n",
    "        # if it hasn't, check if the top hit is the correct match\n",
    "        query = line.split()[0]\n",
    "        if query not in seen:\n",
    "            # get the top hit\n",
    "            top_hit = line.split()[1]\n",
    "            # get the correct match for the query\n",
    "            val = correct_matches.get(query, None)\n",
    "            if val is not None and top_hit == val:\n",
    "                correct += 1\n",
    "        seen.add(query)\n",
    "    \n",
    "os.remove(f\"foldseek_results.m8\")\n",
    "recall = correct / len(correct_matches)\n",
    "print(f\"Foldseek Time taken: {total_time} seconds\")\n",
    "print(f\"Foldseek recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gLM2 Benchmark\n",
    "glm2_model_name = \"tattabio/gLM2_650M_embed\"\n",
    "model = AutoModel.from_pretrained(glm2_model_name, trust_remote_code=True)\n",
    "model = model.eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(glm2_model_name, trust_remote_code=True)\n",
    "\n",
    "def infer_fn(examples, seq_col=\"Seq1\"):\n",
    "    sequences = examples[seq_col]\n",
    "    inputs = tokenizer(\n",
    "        sequences, return_tensors=\"pt\", padding=True, \n",
    "    )\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    attention_mask = inputs[\"attention_mask\"].bool()\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        outputs = model(inputs['input_ids'], attention_mask=attention_mask)\n",
    "        hidden = outputs.pooler_output.float()\n",
    "        hidden = torch.nn.functional.normalize(hidden, dim=-1)\n",
    "    return {f\"{seq_col}_features\": hidden.cpu().numpy()}\n",
    "\n",
    "start_time = time.time()\n",
    "glm2_ds = dataset.map(lambda x: infer_fn(x, seq_col=\"Seq1\"), batched=True, batch_size=64)\n",
    "glm2_ds = glm2_ds.map(lambda x: infer_fn(x, seq_col=\"Seq2\"), batched=True, batch_size=64)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"GLM2 inference time: {total_time:.2f} seconds\")\n",
    "glm2_ds.set_format(type=\"numpy\")\n",
    "\n",
    "start_time = time.time()\n",
    "glm2_ds = glm2_ds.add_faiss_index(column=\"Seq1_features\", metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "seq2_features = glm2_ds[\"Seq2_features\"]\n",
    "scores, hits = glm2_ds.get_nearest_examples_batch(\"Seq1_features\", seq2_features, k=1)\n",
    "hits = [hit['Seq1_accession'] for hit in hits]\n",
    "gt = glm2_ds['Seq1_accession']\n",
    "accuracy = sum([1 for hit, gt in zip(hits, gt) if hit == gt]) / len(gt)\n",
    "\n",
    "print(f\"GLM2 accuracy: {accuracy:.4f}\")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"GLM2 search time: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM2 Benchmark\n",
    "esm2_model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "model = AutoModel.from_pretrained(esm2_model_name)\n",
    "model = model.eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(esm2_model_name)\n",
    "\n",
    "def infer_fn(examples, seq_col=\"Seq1\", layer='mid'):\n",
    "    sequences = examples[seq_col]\n",
    "    inputs = tokenizer(\n",
    "        sequences, return_tensors=\"pt\", padding=True,\n",
    "    )\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    attention_mask = inputs[\"attention_mask\"].bool()\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        outputs = model(inputs['input_ids'], attention_mask=attention_mask, output_hidden_states=True)\n",
    "        layer_idx = len(model.encoder.layer) // 2 - 1 if layer == 'mid' else -1\n",
    "        hiddens = outputs.hidden_states[layer_idx].float()\n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "        # Mean pool\n",
    "        hidden = torch.where(mask, hiddens, 0.0)\n",
    "        hidden = torch.sum(hidden, 1) / torch.sum(mask, dim=1, dtype=hidden.dtype)\n",
    "        hidden = torch.nn.functional.normalize(hidden, dim=-1)\n",
    "    return {f\"{seq_col}_features\": hidden.cpu().numpy()}\n",
    "\n",
    "start_time = time.time()\n",
    "esm2_ds = dataset.map(lambda x: infer_fn(x, seq_col=\"Seq1\"), batched=True, batch_size=64)\n",
    "esm2_ds = esm2_ds.map(lambda x: infer_fn(x, seq_col=\"Seq2\"), batched=True, batch_size=64)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"ESM2 inference time: {total_time:.2f} seconds\")\n",
    "esm2_ds.set_format(type=\"numpy\")\n",
    "\n",
    "start_time = time.time()\n",
    "esm2_ds = esm2_ds.add_faiss_index(column=\"Seq1_features\", metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "seq2_features = esm2_ds[\"Seq2_features\"]\n",
    "scores, hits = esm2_ds.get_nearest_examples_batch(\"Seq1_features\", seq2_features, k=1)\n",
    "hits = [hit['Seq1_accession'] for hit in hits]\n",
    "gt = esm2_ds['Seq1_accession']\n",
    "accuracy = sum([1 for hit, gt in zip(hits, gt) if hit == gt]) / len(gt)\n",
    "\n",
    "print(f\"ESM2 accuracy: {accuracy:.4f}\")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"ESM2 search time: {total_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".glm2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
